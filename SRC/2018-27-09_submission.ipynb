{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web log challenge\n",
    "Notes:\n",
    "1. This is executed on a jupyter notebook running in spark shell (Windows). Hence, sc(SQLContext) is already initialized in the enviornment (with all the information about the spark cluster)\n",
    "2. The shell command to execute the jupyter notebook with this configuration on a Windows machine where pyspark is installed - `pyspark.cmd local[n] --conf spark.network.timeout=10000000` where 'n' is the number of local clusters. The timeout has been set to prevent crash issues that I was facing while training the models\n",
    "3. The pyspark installation instructions for Windows can be found here -  https://medium.com/@GalarnykMichael/install-spark-on-windows-pyspark-4498a5d8d66c\n",
    "4. The code needs the extracted log file in the '../data/'directory. Alternatively, the path can be changed in chunk 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing functions and libraries\n",
    "\n",
    "import shlex #required for easy splits of data using delimiters and respecting quotes\n",
    "from pyspark.sql import functions as ps_fun  \n",
    "from pyspark.sql.types import TimestampType,IntegerType, StringType\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import FeatureHasher, Bucketizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define the coloumn names for the dataset. Taken from amazon documentation and some changes made for easy readibility/parsing\n",
    "col_names = ['timestamp','elb'\n",
    ",'client_ip_port'\n",
    ",'backend_ip_port'\n",
    ",'request_processing_time'\n",
    ",'backend_processing_time'\n",
    ",'response_processing_time'\n",
    ",'elb_status_code'\n",
    ",'backend_status_code'\n",
    ",'received_bytes'\n",
    ",'sent_bytes'\n",
    ",'request'\n",
    ",'user_agent'\n",
    ",'ssl_cipher'\n",
    ",'ssl_protocol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the data had a few rows where the quotation was present inside the data field. This was leading to more number of columns \n",
    "#being parsed and hence leading to errors. The below function has a tolerance for such cases. For such problematic rows, \n",
    "# we return a list of Nones which are filtered later. Since the row count for such rows is low, having such an approach will not\n",
    "#cause any siginificant changes in the outcomes. However, if the row count for such rows is high, then we might need a different\n",
    "#approach for inclusion of such rows\n",
    "def split_try(line,cols_expected):\n",
    "        try:\n",
    "            ret=shlex.split(line)\n",
    "        except:\n",
    "            ret=[None for i in range(0,cols_expected)]\n",
    "        return(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read the text files as an RDD\n",
    "#Apply the splitting function written above to give the split columns. \n",
    "data1 = sc.textFile('../data/2015_07_22_mktplace_shop_web_log_sample.log').map(lambda line:split_try(line,cols_expected=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converting the RDD to a Spark DataFrame for easy manipulation, transformation and analysis\n",
    "#.cache() stores the DataFrame in each of the clusters in memory. This will lead to faster results. However, this may not be\n",
    "#feasible for big datasets due to memory constraints.\n",
    "data = data1.toDF(col_names).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#filtering the data to remove NULL client_ip_port (NULL client_ip_port will be a result of the splitting function \n",
    "#used for problematic rows)\n",
    "data = data.dropna(subset='client_ip_port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#various string split operations:\n",
    "#1) to get IP from IP + port. The same IP might be using different ports for connection, but should be recorded as the same user\n",
    "#2) To get the url from url+agrs - The same url might be called with different arguments but as per the details given, they \n",
    "#should be counted as a single hit\n",
    "\n",
    "#get IP from IP+port\n",
    "split_col_ip = ps_fun.split(data['client_ip_port'], ':')\n",
    "data = data.withColumn('client_ip',split_col_ip.getItem(0))\n",
    "\n",
    "#get URL from URL + args\n",
    "split_col_url = ps_fun.split(data['request'],' ')\n",
    "data = data.withColumn('full_url',split_col_url.getItem(1))\n",
    "split_col_url_2 = ps_fun.split(data['full_url'],'\\\\?')\n",
    "data = data.withColumn('full_url_without_args',split_col_url_2.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#specify the time out for sessions in seconds\n",
    "time_out=15*60 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing & Analytical goals:\n",
    "1) Sessionize the web log by IP. Sessionize = aggregrate all page hits by visitor/IP during a session\n",
    "\n",
    "Approach:\n",
    "1. I have used spark functionalities as much as possible\n",
    "2. Played around with different window approaches including last value etc. However, spark will not update the same variable and use in the next window so that did not work. Used cumsum approach instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating a lagged variable for timestamp. last_session_ts gives the timestamp of the previous call for the same user\n",
    "window_1 = Window.partitionBy(\"client_ip\").orderBy(\"timestamp\")\n",
    "data = data.withColumn('last_session_ts',ps_fun.lag('timestamp').over(window_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculating the difference between the last hit and current hit in seconds\n",
    "#Some issues I faced:\n",
    "#1) IF you notice, the below time stamp ignores the milliseconds and the TZ. unix_timestamp was not reading the timestamp \n",
    "#properly if I gave the millisecond strings and TZ strings. \n",
    "#However, for this problem this may not be an issue as we are interested in time difference in seconds and all the \n",
    "#ts records in a static time zone (Z)\n",
    "#2) I was able to read the time zones properly using pd.to_datetime() and creating it into a udf but I wanted to stick purely to\n",
    "# spark functions.\n",
    "# Tried with the to_timestamp() function too. But did not spend a lot of time on it as milliseconds and TZ are not very important \n",
    "#for this problem\n",
    "timeFmt = \"yyyy-MM-dd'T'HH:mm:ss\" \n",
    "time = ps_fun.unix_timestamp('timestamp',format=timeFmt)\n",
    "timeDiff = (ps_fun.unix_timestamp('timestamp', format=timeFmt)\n",
    "            - ps_fun.unix_timestamp('last_session_ts', format=timeFmt))\n",
    "data = data.withColumn(\"diff_time\", ps_fun.coalesce(timeDiff,ps_fun.lit(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assigning session ID to IPs \n",
    "#1) Create a flag if diff_time greater than time_out\n",
    "#2) For each client_ip and each row, take the sum of all flags till the row (row included). Window.unboundedPreceding is the \n",
    "#dyanmic index which helps us take all the preceeding rows. This is done on data ordered in ascending by timestamp\n",
    "#3) The sum(rather cumsum) is the index of the session_counter for that IP. Combination of IP + session_counter is the unique\n",
    "#session ID\n",
    "window_3 = Window.partitionBy(\"client_ip\").orderBy(\"timestamp\").rowsBetween(Window.unboundedPreceding,0) \n",
    "data = data.withColumn('is_new',ps_fun.when(ps_fun.col('diff_time')>time_out,1).otherwise(0))\n",
    "data = data.withColumn('session_counter',ps_fun.sum('is_new').over(window_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating the client_session as the unique identifier. Defined as concatentation of client_ip and session_counter\n",
    "data=data.withColumn('client_session',ps_fun.concat(ps_fun.col('client_ip'),ps_fun.lit('____'),ps_fun.col('session_counter')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing & Analytical goals:\n",
    "2. Determine the average session time\n",
    "3. Determine unique URL visits per session. To clarify, count a hit to a unique URL only once per session.\n",
    "4. Find the most engaged users, ie the IPs with the longest session times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Aggregating the data by client_sesson to compute session level metrics\n",
    "data_session = data.groupBy([\"client_session\",'client_ip','session_counter']).agg(\n",
    "    ps_fun.min(\"timestamp\").alias(\"ts_first\"), \n",
    "    ps_fun.max(\"timestamp\").alias(\"ts_last\"),\n",
    "    ps_fun.count(\"*\").alias(\"calls\"),\n",
    "    ps_fun.countDistinct(\"full_url_without_args\").alias('n_pages')\n",
    " ).withColumn('time_spent',(ps_fun.unix_timestamp('ts_last',timeFmt) - ps_fun.unix_timestamp('ts_first',timeFmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|     avg(n_pages)|   avg(time_spent)|\n",
      "+-----------------+------------------+\n",
      "|8.044227906347272|100.75318265890739|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#average time spent per session and average URL per visit\n",
    "data_session.agg({'time_spent':'mean',\n",
    "                'n_pages':'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+\n",
      "|   client_session|n_pages|\n",
      "+-----------------+-------+\n",
      "|1.186.143.37____0|      2|\n",
      "|1.187.164.29____0|      8|\n",
      "|  1.22.41.76____0|      5|\n",
      "| 1.23.208.26____0|      4|\n",
      "| 1.23.208.26____1|      1|\n",
      "| 1.23.36.184____0|      4|\n",
      "|   1.38.19.8____0|      1|\n",
      "|  1.38.20.34____0|     14|\n",
      "|  1.39.13.13____0|      2|\n",
      "| 1.39.32.249____0|      4|\n",
      "| 1.39.32.249____1|      2|\n",
      "|  1.39.32.59____0|      1|\n",
      "| 1.39.33.153____0|      6|\n",
      "|  1.39.33.33____0|      2|\n",
      "|  1.39.33.77____0|      2|\n",
      "|  1.39.33.77____1|      4|\n",
      "|   1.39.34.4____0|      1|\n",
      "|  1.39.40.43____0|      2|\n",
      "|  1.39.60.37____0|     31|\n",
      "|  1.39.61.53____0|     19|\n",
      "+-----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#url visits for each session\n",
    "data_session.select(['client_session','n_pages']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_by_ip=data_session.groupBy('client_ip').agg(\\\n",
    "                                                 ps_fun.mean('time_spent').alias('average_time_spent'),\\\n",
    "                                                 ps_fun.sum('time_spent').alias('total_time_spent'),\\\n",
    "                                                 ps_fun.count('*').alias('sessions')\\\n",
    "                                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------------+--------+\n",
      "|     client_ip|average_time_spent|total_time_spent|sessions|\n",
      "+--------------+------------------+----------------+--------+\n",
      "|103.29.159.138|            2065.0|            2065|       1|\n",
      "|125.16.218.194|            2065.0|            2065|       1|\n",
      "|  14.99.226.79|            2063.0|            2063|       1|\n",
      "| 122.169.141.4|            2060.0|            2060|       1|\n",
      "| 14.139.220.98|            2058.0|            2058|       1|\n",
      "|117.205.158.11|            2057.0|            2057|       1|\n",
      "|  111.93.89.14|            2055.0|            2055|       1|\n",
      "|  182.71.63.42|            2051.0|            2051|       1|\n",
      "| 223.176.3.130|            2048.0|            2048|       1|\n",
      "|183.82.103.131|            2042.0|            2042|       1|\n",
      "+--------------+------------------+----------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#top 10 users by average time spent \n",
    "data_by_ip.sort(ps_fun.desc('average_time_spent')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+----------------+--------+\n",
      "|     client_ip|average_time_spent|total_time_spent|sessions|\n",
      "+--------------+------------------+----------------+--------+\n",
      "| 220.226.206.7| 522.6923076923077|            6795|      13|\n",
      "| 119.81.61.166| 683.6666666666666|            6153|       9|\n",
      "|  52.74.219.71|             525.8|            5258|      10|\n",
      "| 54.251.151.39|             523.6|            5236|      10|\n",
      "|121.58.175.128|             498.8|            4988|      10|\n",
      "| 106.186.23.95|             493.1|            4931|      10|\n",
      "|  125.19.44.66|             465.3|            4653|      10|\n",
      "| 54.169.191.85|           577.625|            4621|       8|\n",
      "|  207.46.13.22|             453.5|            4535|      10|\n",
      "|180.179.213.94| 501.3333333333333|            4512|       9|\n",
      "+--------------+------------------+----------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#top 10 users by total time spent \n",
    "data_by_ip.sort(ps_fun.desc('total_time_spent')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+---------------+--------------------+--------------------+-----+-------+----------+--------------+\n",
      "|    client_session|    client_ip|session_counter|            ts_first|             ts_last|calls|n_pages|time_spent|max_time_spent|\n",
      "+------------------+-------------+---------------+--------------------+--------------------+-----+-------+----------+--------------+\n",
      "| 52.74.219.71____4| 52.74.219.71|              4|2015-07-22T10:30:...|2015-07-22T11:04:...|11609|   9530|      2069|          2069|\n",
      "|119.81.61.166____4|119.81.61.166|              4|2015-07-22T10:30:...|2015-07-22T11:04:...| 1818|   1739|      2069|          2069|\n",
      "|106.186.23.95____4|106.186.23.95|              4|2015-07-22T10:30:...|2015-07-22T11:04:...| 2848|   2731|      2069|          2069|\n",
      "+------------------+-------------+---------------+--------------------+--------------------+-----+-------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sessions with maximum time spent\n",
    "data_session.withColumn('max_time_spent',ps_fun.max('time_spent').over(Window.partitionBy())).filter('time_spent=max_time_spent').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts_format_minute = \"yyyy-MM-dd'T'HH:mm\"\n",
    "ts_format_second = \"yyyy-MM-dd'T'HH:mm:ss\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the session length/ unique URLs for a given IP\n",
    "This problem can potentially be solved by using two seperate models - one for repeat user and one for first time users. The repeat users model can use features meant for new users also as they may add to the predictive power of the model\n",
    "\n",
    "I have given below the features that can be used for determining session length. The same method/approach may also be extended to determine unique URLs (features will have URL counts etc. instead of session time, the target will also change). Anyhow, both will be fairly correlated.\n",
    "\n",
    "#### Returning users:\n",
    "There is lot of scope in using information from the past behavior of the customer. Namely we can derive the following features leveraging the past history (limited set, can be expanded further):\n",
    "1. time_spent_last_session - \n",
    "2. time_spent_in_the_day_till_now - To capture that users spend a fixed amount of time online everyday. Can also interact with day of the week which will capture users spending variable amounts of time on weekdays versus weekends\n",
    "3. current_and_last_session_time_difference - Sessions happening very close to each other may lead to shorter sessions\n",
    "4. average_time_spent_all_past_sessions\n",
    "\n",
    "#### New users:\n",
    "For new users we have to rely on the overall features for example(limited set, can be expanded further):\n",
    "1. avg_session_duration_new_users\n",
    "2. avg_session_duration_new_users_same_hour\n",
    "3. avg_session_duration_new_users_same_day\n",
    "4. avg_session_duration_new_users_same_day_hour\n",
    "5. source_of_the_user\n",
    "6. country_of_the_IP\n",
    "7. hour_of_the_day\n",
    "8. day_of_the_week\n",
    "\n",
    "I have illustrated the training of returning users model (due to time constraints); however, the feature engineering has been done for both.\n",
    "\n",
    "**Note:** I am limiting myself to the use of web log only. If we have more data there are several features that can be used (sample list):\n",
    "1. source - organic versus marketing versus search. An organic user may spend more time since he specifically browsed on the site for some purpose. \n",
    "2. lat/long of user - Can use IP to determine that. Can leverage that information to get location specific trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#derive returning users features:\n",
    "data_session_2 = data_session.withColumn('date_last',ps_fun.to_date('ts_last', 'yyyy-MM-dd'))\\\n",
    "                             .withColumn('date_first',ps_fun.to_date('ts_first', 'yyyy-MM-dd'))\\\n",
    "                             .withColumn('date',ps_fun.greatest('date_last','date_first'))\\\n",
    "                             .withColumn('datetime',ps_fun.to_timestamp('ts_first',ts_format_second))\\\n",
    "                            .withColumn('hour_of_the_day',ps_fun.hour('datetime'))\\\n",
    "                            .withColumn('day_of_the_week',ps_fun.dayofweek('datetime'))\\\n",
    "                             .withColumn('ts_first',ps_fun.unix_timestamp('ts_first',ts_format_minute))\\\n",
    "                             .withColumn('ts_last',ps_fun.unix_timestamp('ts_last',ts_format_minute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#derive existing user features\n",
    "#To compute the IP level features without restricting to a date. The lower bound of rows is set to last 500 to ignore\n",
    "#data from very old sessions. 500 is an ADHOC choice and can be improved\n",
    "window_3 = Window.partitionBy('client_ip').orderBy('session_counter').rowsBetween(-500,-1)\n",
    "#Set a partition by IP and date to compute the past activity in the same day. Here we do not put a lower bound as all \n",
    "#activity in a day must be included\n",
    "window_4 = Window.partitionBy(['client_ip','date']).orderBy('session_counter').rowsBetween(Window.unboundedPreceding,-1)\n",
    "\n",
    "#actual application of aggregations over different windows\n",
    "data_returning = data_session_2.withColumn('time_spent_last_session',ps_fun.last('time_spent').over(window_3))\\\n",
    "                             .withColumn('current_and_last_session_time_difference',ps_fun.col('ts_first')-ps_fun.last('ts_last').over(window_3))\\\n",
    "                             .withColumn('average_time_spent_all_past_sessions',ps_fun.mean('time_spent').over(window_3))\\\n",
    "                             .withColumn('time_spent_in_the_day_till_now',ps_fun.coalesce(ps_fun.sum('time_spent').over(window_3),ps_fun.lit(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#derive new user features\n",
    "#Setting different partitions to compute features at different levels - overall, by hour, by day of the week and by day of the\n",
    "# week and hour. Restricted to last 5000 sessions as we want to limit influence from very old sessions. 5000 is ADHOC and can be\n",
    "#improved\n",
    "window_5 = Window.partitionBy().orderBy('ts_last').rowsBetween(-5000,-1)\n",
    "window_6 = Window.partitionBy('hour_of_the_day').orderBy('ts_last').rowsBetween(-5000,-1)\n",
    "window_7 = Window.partitionBy('day_of_the_week').orderBy('ts_last').rowsBetween(-5000,-1)\n",
    "window_8 = Window.partitionBy(['day_of_the_week','hour_of_the_day']).orderBy('ts_last').rowsBetween(-5000,-1)\n",
    "\n",
    "#computing features\n",
    "data_time_trends = data_session_2\\\n",
    "        .withColumn('avg_session_duration',ps_fun.mean('time_spent').over(window_5))\\\n",
    "        .withColumn('avg_session_duration_same_hour',ps_fun.mean('time_spent').over(window_6))\\\n",
    "        .withColumn('avg_session_duration_same_day',ps_fun.mean('time_spent').over(window_7))\\\n",
    "        .withColumn('avg_session_duration_same_day_hour',ps_fun.mean('time_spent').over(window_8))\\\n",
    "        .select('client_session',\n",
    "                'avg_session_duration',\n",
    "                'avg_session_duration_same_hour',\n",
    "                'avg_session_duration_same_day',\n",
    "                'avg_session_duration_same_day_hour'\n",
    "                #'client_ip',\n",
    "                #'ts_first',\n",
    "                #'ts_last'\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#join new user and returning user features\n",
    "data_all_features = data_returning.join(data_time_trends,on='client_session',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining features for the models along with the target variable\n",
    "feature_list_returning=[\n",
    "'time_spent_last_session',\n",
    "'current_and_last_session_time_difference',\n",
    "'average_time_spent_all_past_sessions',\n",
    "'time_spent_in_the_day_till_now',\n",
    "'avg_session_duration',\n",
    "'avg_session_duration_same_hour',\n",
    "'avg_session_duration_same_day',\n",
    "'avg_session_duration_same_day_hour',\n",
    "'hour_of_the_day',\n",
    "'day_of_the_week'\n",
    "]\n",
    "feature_list_new = [\n",
    "'avg_session_duration',\n",
    "'avg_session_duration_same_hour',\n",
    "'avg_session_duration_same_day',\n",
    "'avg_session_duration_same_day_hour',\n",
    "'hour_of_the_day',\n",
    "'day_of_the_week'   \n",
    "]\n",
    "target = 'time_spent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#seperating data into returning and new users\n",
    "# dropna can be avoided by better imputation of NAs (some may be known missings)\n",
    "data_returning = data_all_features.filter('session_counter>0').select(feature_list_returning+[target]).withColumnRenamed(target,'label').dropna()\n",
    "data_new = data_all_features.filter('session_counter==0').select(feature_list_new+[target]).withColumnRenamed(target,'label').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------------------------+------------------------------------+------------------------------+--------------------+------------------------------+-----------------------------+----------------------------------+---------------+---------------+-----+\n",
      "|time_spent_last_session|current_and_last_session_time_difference|average_time_spent_all_past_sessions|time_spent_in_the_day_till_now|avg_session_duration|avg_session_duration_same_hour|avg_session_duration_same_day|avg_session_duration_same_day_hour|hour_of_the_day|day_of_the_week|label|\n",
      "+-----------------------+----------------------------------------+------------------------------------+------------------------------+--------------------+------------------------------+-----------------------------+----------------------------------+---------------+---------------+-----+\n",
      "|                     52|                                    1080|                                52.0|                            52|              33.504|                        33.504|                       33.504|                            33.504|             16|              4|   40|\n",
      "|                     16|                                   33300|                                16.0|                            16|              60.564|                        60.564|                       60.564|                            60.564|             16|              4|  208|\n",
      "|                     34|                                    1740|                                34.0|                            34|             81.6746|                       81.6746|                      81.6746|                           81.6746|             16|              4|   57|\n",
      "|                      0|                                   25560|                                 0.0|                             0|             32.1216|            28.497682258744206|                      32.1216|                28.497682258744206|             17|              4|  156|\n",
      "|                     16|                                    1260|                                16.0|                            16|             72.8756|                       72.2748|                      72.8756|                           72.2748|             18|              4|    7|\n",
      "|                      0|                                    1200|                                 0.0|                             0|             51.7856|                       51.7856|                      51.7856|                           51.7856|             18|              4|  152|\n",
      "|                      9|                                    6420|                                 9.0|                             9|             83.5636|             3.296103896103896|                      83.5636|                 3.296103896103896|             18|              4|    0|\n",
      "|                    722|                                    4800|                               722.0|                           722|             43.2128|              20.7518617813524|                      43.2128|                  20.7518617813524|             17|              4|    0|\n",
      "|                    190|                                   13680|                               190.0|                           190|            158.6236|                      158.6236|                     158.6236|                          158.6236|             10|              4|    0|\n",
      "|                     16|                                   14760|                                16.0|                            16|            243.4578|            10.572202166064981|                      268.068|                10.572202166064981|             11|              4|   27|\n",
      "|                    330|                                   26460|                               189.0|                           378|             84.9764|            3.4391304347826086|                      84.9624|                3.4391304347826086|             18|              4|    4|\n",
      "|                    252|                                    5160|                              1155.5|                          2311|             78.1562|                       78.1562|                      78.1562|                           78.1562|             17|              4|  289|\n",
      "|                      1|                                    6120|                                 1.0|                             1|             38.3794|             29.08284023668639|                      38.3794|                 29.08284023668639|              6|              4|  126|\n",
      "|                     24|                                    6420|                                97.5|                           195|             55.7386|            14.227090178897408|                      55.7386|                14.227090178897408|             18|              4|  119|\n",
      "|                      0|                                   12660|                                 0.0|                             0|             80.8126|                       80.8126|                      80.8126|                           80.8126|             10|              4|  447|\n",
      "|                   2069|                                   18360|                               616.0|                          3080|            200.5562|                      200.5562|                     200.5562|                          200.5562|             16|              4|  876|\n",
      "|                    207|                                     960|                               207.0|                           207|             37.5982|                       37.5982|                      37.5982|                           37.5982|             16|              4|  128|\n",
      "|                     13|                                   39780|                                13.0|                            13|              49.741|            15.840766331658292|                       49.741|                15.840766331658292|             18|              4|    0|\n",
      "|                     34|                                     900|                                34.0|                            34|            153.6414|                      153.6414|                     153.6414|                          153.6414|             10|              4|    0|\n",
      "|                     28|                                   22140|                                28.0|                            28|              72.472|                        72.472|                       72.472|                            72.472|             16|              4|  205|\n",
      "+-----------------------+----------------------------------------+------------------------------------+------------------------------+--------------------+------------------------------+-----------------------------+----------------------------------+---------------+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show sample rows\n",
    "data_returning.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining the modelling pipeline\n",
    "#faced major issues here in terms of the runtime on a small dataset. I am sure I am missing something in terms of distributing \n",
    "#the job. Will need to dig in and figure out what engineering can be done to reduce the runtime and make the process efficient\n",
    "rf = RandomForestRegressor(featuresCol='features')\n",
    "hasher = FeatureHasher(inputCols=feature_list_returning,\n",
    "                       outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[hasher,rf])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [1]) \\\n",
    "    .addGrid(rf.numTrees,[50])\\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=2,\n",
    "                         seed=123)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(dataset=data_returning.cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[245.5129805939468]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the average CV metric. I am limiting myself to just training due to time constraints. Will not go into validation, improvement,\n",
    "#analyzing importances, picturing partials, and finally setting a scoring pipeline\n",
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Questions:\n",
    "\n",
    "1) Predict the expected load (requests/second) in the next minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create time features\n",
    "data = data.withColumn('timestamp_minute',ps_fun.to_timestamp(ps_fun.col('timestamp'),ts_format_minute)).\\\n",
    "            withColumn('hour_of_the_day',ps_fun.hour('timestamp_minute')).\\\n",
    "            withColumn('timestamp_second',ps_fun.to_timestamp(ps_fun.col('timestamp'),ts_format_second)).\\\n",
    "            withColumn('second',ps_fun.to_timestamp(ps_fun.second('timestamp_second'))).\\\n",
    "            withColumn('minute',ps_fun.minute('timestamp_second')).\\\n",
    "            withColumn('min-sec',ps_fun.concat('minute',ps_fun.lit('-'),'second'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|hour_of_the_day|count|\n",
      "+---------------+-----+\n",
      "|             16|  820|\n",
      "|             10|  813|\n",
      "|             11|  337|\n",
      "|             17|  309|\n",
      "|              5|  305|\n",
      "|             18|  304|\n",
      "|              2|  301|\n",
      "|             21|  301|\n",
      "|              6|  298|\n",
      "|              9|  258|\n",
      "|             13|   89|\n",
      "|             19|   60|\n",
      "|             15|   59|\n",
      "|              7|    8|\n",
      "|             12|    7|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the table below aims to illustrate the unique min-sec combinations within each hour. It shows that the data is not collected \n",
    "#for each minute within an hour. Next we try to see that if the data is collected for a minute, is it collected for all seconds\n",
    "#in that minute\n",
    "data.groupby('hour_of_the_day').agg(ps_fun.countDistinct('min-sec').alias('count')).sort(ps_fun.desc('count')).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+\n",
      "|   timestamp_minute|count_unique_second|requests|\n",
      "+-------------------+-------------------+--------+\n",
      "|2015-07-22 16:43:00|                 60|   26457|\n",
      "|2015-07-22 16:21:00|                 60|   24841|\n",
      "|2015-07-22 10:33:00|                 60|   24642|\n",
      "|2015-07-22 10:46:00|                 60|   24321|\n",
      "|2015-07-22 16:11:00|                 60|   24120|\n",
      "|2015-07-22 16:42:00|                 60|   24095|\n",
      "|2015-07-22 09:01:00|                 60|   24076|\n",
      "|2015-07-22 10:38:00|                 60|   23944|\n",
      "|2015-07-22 11:01:00|                 60|   23898|\n",
      "|2015-07-22 09:02:00|                 60|   23411|\n",
      "|2015-07-22 10:47:00|                 60|   23387|\n",
      "|2015-07-22 10:36:00|                 60|   23301|\n",
      "|2015-07-22 09:03:00|                 60|   23012|\n",
      "|2015-07-22 10:37:00|                 60|   22896|\n",
      "|2015-07-22 10:48:00|                 60|   22885|\n",
      "|2015-07-22 11:02:00|                 60|   22847|\n",
      "|2015-07-22 16:22:00|                 60|   22665|\n",
      "|2015-07-22 17:42:00|                 60|   22028|\n",
      "|2015-07-22 16:12:00|                 60|   21987|\n",
      "|2015-07-22 10:31:00|                 60|   21513|\n",
      "|2015-07-22 17:41:00|                 60|   21103|\n",
      "|2015-07-22 16:41:00|                 60|   21058|\n",
      "|2015-07-22 16:24:00|                 60|   21052|\n",
      "|2015-07-22 16:13:00|                 60|   21001|\n",
      "|2015-07-22 10:34:00|                 53|   20915|\n",
      "|2015-07-22 10:32:00|                 60|   20489|\n",
      "|2015-07-22 11:03:00|                 60|   20207|\n",
      "|2015-07-22 11:04:00|                 58|   20022|\n",
      "|2015-07-22 16:14:00|                 53|   19875|\n",
      "|2015-07-22 16:23:00|                 60|   19479|\n",
      "|2015-07-22 17:43:00|                 60|   18820|\n",
      "|2015-07-22 18:03:00|                 60|   18247|\n",
      "|2015-07-22 10:39:00|                 48|   17987|\n",
      "|2015-07-22 10:49:00|                 53|   17830|\n",
      "|2015-07-22 09:04:00|                 44|   17619|\n",
      "|2015-07-22 17:44:00|                 60|   17216|\n",
      "|2015-07-22 16:44:00|                 49|   16914|\n",
      "|2015-07-22 18:04:00|                 60|   16340|\n",
      "|2015-07-22 18:02:00|                 60|   15907|\n",
      "|2015-07-22 18:01:00|                 60|   14476|\n",
      "|2015-07-22 05:13:00|                 60|   14463|\n",
      "|2015-07-22 06:56:00|                 60|   13473|\n",
      "|2015-07-22 06:57:00|                 60|   13289|\n",
      "|2015-07-22 11:00:00|                 34|   13025|\n",
      "|2015-07-22 16:10:00|                 36|   12993|\n",
      "|2015-07-22 06:55:00|                 55|   12531|\n",
      "|2015-07-22 10:30:00|                 38|   12441|\n",
      "|2015-07-22 05:14:00|                 60|   12289|\n",
      "|2015-07-22 05:12:00|                 60|   12255|\n",
      "|2015-07-22 06:59:00|                 60|   11967|\n",
      "|2015-07-22 06:58:00|                 60|   11921|\n",
      "|2015-07-22 09:00:00|                 34|   11881|\n",
      "|2015-07-22 10:35:00|                 37|   11868|\n",
      "|2015-07-22 05:11:00|                 60|   11670|\n",
      "|2015-07-22 10:45:00|                 36|   11570|\n",
      "|2015-07-22 16:40:00|                 37|   11474|\n",
      "|2015-07-22 05:10:00|                 56|   10099|\n",
      "|2015-07-22 16:20:00|                 36|    9981|\n",
      "|2015-07-22 17:40:00|                 37|    9727|\n",
      "|2015-07-22 17:45:00|                 29|    8472|\n",
      "|2015-07-22 18:05:00|                 29|    8347|\n",
      "|2015-07-22 18:00:00|                 35|    7472|\n",
      "|2015-07-22 02:41:00|                 60|    6787|\n",
      "|2015-07-22 21:06:00|                 60|    6128|\n",
      "|2015-07-22 02:42:00|                 60|    5586|\n",
      "|2015-07-22 21:08:00|                 60|    5163|\n",
      "|2015-07-22 21:09:00|                 60|    5061|\n",
      "|2015-07-22 02:43:00|                 60|    4734|\n",
      "|2015-07-22 02:40:00|                 54|    4681|\n",
      "|2015-07-22 02:44:00|                 60|    4680|\n",
      "|2015-07-22 21:05:00|                 33|    4035|\n",
      "|2015-07-22 21:07:00|                 60|    3300|\n",
      "|2015-07-22 21:10:00|                 28|    2465|\n",
      "|2015-07-22 16:25:00|                  6|    1980|\n",
      "|2015-07-22 05:15:00|                  8|    1533|\n",
      "|2015-07-22 07:00:00|                  8|    1282|\n",
      "|2015-07-22 02:45:00|                  7|     323|\n",
      "|2015-07-22 15:53:00|                  4|      51|\n",
      "|2015-07-22 15:50:00|                 18|      50|\n",
      "|2015-07-22 15:52:00|                 22|      48|\n",
      "|2015-07-22 13:06:00|                 20|      45|\n",
      "|2015-07-22 13:08:00|                 17|      42|\n",
      "|2015-07-22 13:07:00|                 17|      40|\n",
      "|2015-07-22 11:59:00|                 17|      37|\n",
      "|2015-07-22 13:09:00|                 14|      35|\n",
      "|2015-07-22 19:28:00|                 18|      33|\n",
      "|2015-07-22 11:58:00|                 13|      31|\n",
      "|2015-07-22 13:05:00|                 18|      31|\n",
      "|2015-07-22 11:56:00|                 12|      30|\n",
      "|2015-07-22 15:54:00|                  9|      24|\n",
      "|2015-07-22 19:27:00|                 12|      23|\n",
      "|2015-07-22 11:57:00|                 12|      21|\n",
      "|2015-07-22 11:55:00|                 11|      17|\n",
      "|2015-07-22 19:29:00|                 11|      15|\n",
      "|2015-07-22 19:26:00|                 10|      13|\n",
      "|2015-07-22 12:00:00|                  7|      12|\n",
      "|2015-07-22 19:25:00|                  5|       9|\n",
      "|2015-07-22 15:51:00|                  4|       9|\n",
      "|2015-07-22 13:10:00|                  3|       8|\n",
      "|2015-07-22 10:44:00|                  6|       6|\n",
      "+-------------------+-------------------+--------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the below table illustrates that even if the data is collected for a minute, it is not being collected for all the seconds. \n",
    "#Now to make any reliable predictions, the dataset should be systematically generated/sampled. For the purpose of making this \n",
    "#true for this dataset, I am assuming that if for a minute, all the seconds are recorded then all the data within those seconds\n",
    "#are recorded. Hence, the load for that minute is reliable. Thus, for modelling I will take only those minutes that have all\n",
    "#seconds recorded\n",
    "data_counts=data.groupby('timestamp_minute').agg(ps_fun.countDistinct('second').alias('count_unique_second'),ps_fun.count('*').alias('requests')).sort(ps_fun.desc('requests'))\n",
    "data_counts.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#subsetting data for minutes with all seconds recoded. Reason given above\n",
    "data_minute = data_counts.filter('count_unique_second=60')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I have straight away used the the lag to compute requests last minute. A shortcoming here would be that if for a minute \n",
    "#the data is not present, it will take the requests from the last avaialable minute. This is the probably a bad feature for the\n",
    "#dataset at hand but can be quite useful for the full dataset\n",
    "data_minute = data_minute.withColumn('requests_last_minute',ps_fun.lag('requests').over(Window.partitionBy().orderBy('timestamp_minute')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#not doing a lot of feature engineering. A lot of other features can be thought of here\n",
    "data_minute = data_minute.withColumn('hour_of_the_day', ps_fun.hour('timestamp_minute')).\\\n",
    "                          withColumn('day_of_the_week',ps_fun.dayofweek('timestamp_minute'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for predicting something like workload, it is not important for us to be acuurate till the units/tens place; for example,the \n",
    "#workload of 200 and workload of 210 may mean the same thing in terms of infrastructure planning. However, what is essential \n",
    "#to predict is jumps in the workload  - for example 200 versus 1000. Hence, it is logical to bucketize the target variable. \n",
    "#I am choosing 5 buckets here. This, however, should be determined together with the product team to guage the inflexion points\n",
    "# for infra requirement based on workloads\n",
    "gre_histogram = data_minute.select('requests').rdd.flatMap(lambda x: x).histogram(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3300.0, 7931.4, 12562.8, 17194.199999999997, 21825.6, 26457],\n",
       " [8, 5, 6, 11, 19])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#viewing the cutpoints along with their frequency\n",
    "gre_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converting the target variable into buckets using the above cutpoints\n",
    "bucketizer = Bucketizer(splits=[0]+gre_histogram[0]+[float('Inf')],inputCol=\"requests\", outputCol=\"label\")\n",
    "data_minute = bucketizer.setHandleInvalid(\"keep\").transform(data_minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_minute = data_minute.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+--------------------+---------------+---------------+-----+\n",
      "|   timestamp_minute|count_unique_second|requests|requests_last_minute|hour_of_the_day|day_of_the_week|label|\n",
      "+-------------------+-------------------+--------+--------------------+---------------+---------------+-----+\n",
      "|2015-07-22 02:42:00|                 60|    5586|                6787|              2|              4|  1.0|\n",
      "|2015-07-22 02:43:00|                 60|    4734|                5586|              2|              4|  1.0|\n",
      "|2015-07-22 02:44:00|                 60|    4680|                4734|              2|              4|  1.0|\n",
      "|2015-07-22 05:11:00|                 60|   11670|                4680|              5|              4|  2.0|\n",
      "|2015-07-22 05:12:00|                 60|   12255|               11670|              5|              4|  2.0|\n",
      "|2015-07-22 05:13:00|                 60|   14463|               12255|              5|              4|  3.0|\n",
      "|2015-07-22 05:14:00|                 60|   12289|               14463|              5|              4|  2.0|\n",
      "|2015-07-22 06:56:00|                 60|   13473|               12289|              6|              4|  3.0|\n",
      "|2015-07-22 06:57:00|                 60|   13289|               13473|              6|              4|  3.0|\n",
      "|2015-07-22 06:58:00|                 60|   11921|               13289|              6|              4|  2.0|\n",
      "|2015-07-22 06:59:00|                 60|   11967|               11921|              6|              4|  2.0|\n",
      "|2015-07-22 09:01:00|                 60|   24076|               11967|              9|              4|  5.0|\n",
      "|2015-07-22 09:02:00|                 60|   23411|               24076|              9|              4|  5.0|\n",
      "|2015-07-22 09:03:00|                 60|   23012|               23411|              9|              4|  5.0|\n",
      "|2015-07-22 10:31:00|                 60|   21513|               23012|             10|              4|  4.0|\n",
      "|2015-07-22 10:32:00|                 60|   20489|               21513|             10|              4|  4.0|\n",
      "|2015-07-22 10:33:00|                 60|   24642|               20489|             10|              4|  5.0|\n",
      "|2015-07-22 10:36:00|                 60|   23301|               24642|             10|              4|  5.0|\n",
      "|2015-07-22 10:37:00|                 60|   22896|               23301|             10|              4|  5.0|\n",
      "|2015-07-22 10:38:00|                 60|   23944|               22896|             10|              4|  5.0|\n",
      "+-------------------+-------------------+--------+--------------------+---------------+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_minute.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#defining the modelling pipeline\n",
    "#faced major issues here in terms of the runtime on a small dataset. I am sure I am missing something in terms of distributing \n",
    "#the job. Will need to dig in and figure out what engineering can be done to reduce the runtime and make the process efficient\n",
    "lr = LogisticRegression(maxIter=2,family=\"multinomial\")\n",
    "hasher = FeatureHasher(inputCols=['requests_last_minute','hour_of_the_day','day_of_the_week'],\n",
    "                       outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[hasher,lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1])\\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2,\n",
    "                         seed=123)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training the model with cross validation\n",
    "crossval_logistic = crossval.fit(dataset=data_minute.cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2607669082125604]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the average CV metric. I am limiting myself to just training due to time constraints. Will not go into validation, improvement,\n",
    "#analyzing importances, picturing partials, and finally setting scoring pipeline\n",
    "crossval_logistic.avgMetrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
